{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f2c10a",
   "metadata": {},
   "source": [
    "<div style=\"width:90%; margin:0 auto; background-color:#F0F0F0; padding:20px; border-radius:8px; font-family:Arial, sans-serif\">\n",
    "\n",
    "<div style=\"display:flex; align-items:center; justify-content:space-between; margin-bottom:15px\">\n",
    "    <div style=\"width:100px; height:100px\">\n",
    "        <img src=\"data:image/png;base64,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\" style=\"max-width:100%; max-height:100%\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center; margin-bottom:20px\">\n",
    "        <h1 style=\"color:#9E0B0F; font-size:26px; margin-bottom:5px\">Análisis de modelos predictivos en bolsa</h1>\n",
    "        <h3 style=\"color:#D64550; font-size:22px; margin-top:0\">Copyright (C) 2024-2025 MegaStorm Systems</h3>\n",
    "    </div>\n",
    "    <div style=\"width:192px; height:59px\">\n",
    "        <p style=\"max-width:100%; max-height:100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:60%; margin:0 auto; background-color:#E8E8E8; padding:20px; border-radius:10px; border:1px solid #D0D0D0; margin-bottom:20px; color:#333333; line-height:1.5; box-shadow:0 2px 4px rgba(0,0,0,0.05)\">\n",
    "    <p style=\"margin:0; font-size:15px\">    \n",
    "    This software is provided \"as-is\", without any express or implied<br>\n",
    "    warranty. In no event will the authors be held liable for any damages<br>\n",
    "    arising from the use of this software.<br><br>\n",
    "    Permission is granted to anyone to use this software for any purpose,<br>\n",
    "    including commercial applications, and to alter it and redistribute it<br>\n",
    "    freely, subject to the following restrictions:<br><br>\n",
    "    1. The origin of this software must not be misrepresented; you must not<br>\n",
    "    claim that you wrote the original software. If you use this software<br>\n",
    "    in a product, an acknowledgment in the product documentation would be<br>\n",
    "    appreciated but is not required.<br>\n",
    "    2. Altered source versions must be plainly marked as such, and must not be<br>\n",
    "    misrepresented as being the original software.<br>\n",
    "    3. This notice may not be removed or altered from any source distribution.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <h2 style=\"font-size:24px; color:#9E0B0F; margin:0\">Predictor LSTM v3.4</h2>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7b7ad-0674-4820-9e5a-616fb930fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "import argparse\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ampblib import AMPBConfig, processData, generateEvaluation, updateNextDayExog, createReport, getExogVars, reverseTransformPredictions, createModelIdentity, createTFDataset, analyzeTrainingResults, createSequences, setupTensorflowDevice, resetRandomSeeds, calculateValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. PARÁMETROS CONFIGURABLES\n",
    "model_name = \"LSTM\"\n",
    "model_version = \"v3.4\"\n",
    "\n",
    "# Por defecto, permite ejecucion interactiva\n",
    "default_transformation = \"Log\" # \"Log\", \"YeoJohnson\"\n",
    "default_exog_scaling = \"Standard\"   # \"Standard\"\n",
    "default_exog_set_id = 123456    # 1=\"Directos\", 2=\"IndicadoresTecnicos\", 3=\"BigTech\", 4=\"IndicesBursatiles\", 5=\"IndicadoresEconomicos\", 6=\"AnalisisSentimiento\"\n",
    "\n",
    "# Estos son fijos e internos, no los exponemos\n",
    "nombre_archivo = \"NVDA_2015-01-05_2025-05-23_SA.csv\"\n",
    "test_size = 60              # Número de días para el conjunto de test\n",
    "optimize_hyperparams = True # True para optimizar hiperparámetros con Optuna\n",
    "winsorization_value = 0.05  # Aplicar winsorización. Este valor es el mejor encontrado, 0.04 y 0.06 son peores y a mayor valor, mas valores que modidicamos.\n",
    "run_backtesting = False     # True para Backtesting con Walk-Forward\n",
    "fixed_epochs = 40\n",
    "retrain_interval = 5        # Reentrenar modelo completo cada n días en backtesting. 5 es un valor adecuado, acelera la velocidad a cambio de perder un ~6% de rendimiento.\n",
    "n_optuna_trials = 10        # Número de trials para Optuna\n",
    "optuna_debug = False\n",
    "cv_final_evaluation = False  # Cambiar a False para deshabilitar cross-validation\n",
    "n_cv_folds = 5              # Separación de unos 8 meses\n",
    "\n",
    "# Configuración determinista y TensorFlow\n",
    "resetRandomSeeds(tensorflow_deterministic=True)\n",
    "setupTensorflowDevice(use_gpu=False)\n",
    "\n",
    "# Hiperparámetros fijos si optimize_hyperparams = False\n",
    "fixed_sequence_length = 30# 10\n",
    "fixed_lstm_units = [192] #[160]  \n",
    "fixed_dropout_rate = 0.10 #0.05\n",
    "fixed_batch_size = 64\n",
    "fixed_learning_rate = 0.00041 #0.0015783103987152393\n",
    "\n",
    "# Argumentos de línea de comandos  \n",
    "if AMPBConfig.INTERACTIVE:\n",
    "    transformation = default_transformation\n",
    "    exog_scaling = default_exog_scaling\n",
    "    exog_set_id = default_exog_set_id\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description='Ejecuta modelo LSTM.')\n",
    "    parser.add_argument('--transformation', type=str, default=default_transformation, choices=['None', 'Log', 'RetLog', 'YeoJohnson'])\n",
    "    parser.add_argument('--exog_scaling', type=str, default=default_exog_scaling, choices=['None', 'Standard', 'MinMax'])\n",
    "    def valid_exog(x):\n",
    "        if not all(c in '123456' for c in str(x)): \n",
    "            raise argparse.ArgumentTypeError(f\"Solo dígitos 1-6: {x}\")\n",
    "        return x\n",
    "    parser.add_argument('--exog_set_id', type=valid_exog, default=default_exog_set_id)\n",
    "    args = parser.parse_args()\n",
    "    transformation = args.transformation\n",
    "    exog_scaling = args.exog_scaling\n",
    "    exog_set_id = args.exog_set_id\n",
    "\n",
    "AMPBConfig.printHeader(title=f\"Predictor {model_name} {model_version}\", testsize=test_size, \n",
    "                     optimize=optimize_hyperparams, backtesting=run_backtesting, transform=transformation,\n",
    "                     exogscaling=exog_scaling, exogsetid=exog_set_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGA Y PREPARACIÓN DE DATOS\n",
    "datos = pd.read_csv(nombre_archivo)\n",
    "\n",
    "# Seleccionar columnas relevantes\n",
    "mandatory_vars = ['Date','Close', 'Trend']\n",
    "\n",
    "# Obtenemos lista de exogenas\n",
    "exog_vars = getExogVars(exog_set_id)\n",
    "df = datos[mandatory_vars + exog_vars]\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])  \n",
    "\n",
    "# Verificar y mostrar estadísticas básicas de los datos\n",
    "print(f\"Datos en crudo cargados: {len(df)} registros de {df['Date'].min()} a {df['Date'].max()}.\")\n",
    "\n",
    "# Detectar valores faltantes\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "if not missing.empty:\n",
    "    total_before = len(df)\n",
    "    print(\"Valores faltantes por columna (se borrarán estas filas):\")\n",
    "    for col, cnt in missing.items():\n",
    "        print(f\"  • {col}: {cnt} valores faltantes\")\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    total_after = len(df)\n",
    "    removed = total_before - total_after\n",
    "    print(f\"\\nFilas borradas: {removed}\")\n",
    "else:\n",
    "    print(\"No se encontraron valores faltantes.\")\n",
    "\n",
    "# Poner Date como indice\n",
    "df.set_index('Date', inplace=True)\n",
    "#df = df.asfreq('B', method='pad')  # Se asume que se trata de datos bursátiles (días hábiles)\n",
    "\n",
    "print(f\"\\nDatos cargados: {AMPBConfig.COLOR_VALUE}{len(df)}{AMPBConfig.COLOR_RESET} registros. Variables exógenas seleccionadas: {AMPBConfig.COLOR_VALUE}{len(exog_vars)}{AMPBConfig.COLOR_RESET}\\n{exog_vars}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DIVISIÓN ENTRE ENTRENAMIENTO Y TEST\n",
    "\n",
    "# 2A. Se utiliza el 90% de los datos para entrenamiento y el 10% para test\n",
    "#split_index = int(len(df) * 0.90)\n",
    "#df_train = df.iloc[:split_index].copy()\n",
    "#df_test = df.iloc[split_index:].copy()\n",
    "\n",
    "# 2B. Separamos datos de entrenamiento y de test por fecha\n",
    "#split_date = pd.Timestamp('2024-12-01')\n",
    "#df_train = df.loc[:split_date].copy()\n",
    "#df_test = df.loc[split_date:].copy()\n",
    "\n",
    "# 2C. Separamos por numero de dias.\n",
    "df_train = df.iloc[:-test_size]\n",
    "df_test = df.iloc[-test_size:]\n",
    "\n",
    "# Guardar valores originales (inmutables para referencia futura)\n",
    "y_train_original = df_train['Close'].copy()\n",
    "y_test_original = df_test['Close'].copy()\n",
    "X_train_original = df_train[exog_vars].copy()  \n",
    "X_test_original = df_test[exog_vars].copy()\n",
    "\n",
    "# Variables de trabajo (se transformarán/escalarán según configuración)\n",
    "y_train = df_train['Close']\n",
    "y_test  = df_test['Close']\n",
    "X_train = df_train[exog_vars].copy()\n",
    "X_test = df_test[exog_vars].copy()\n",
    "\n",
    "print(f\"\\nDatos divididos:\")\n",
    "print(f\"  Entrenamiento: {AMPBConfig.COLOR_VALUE}{len(y_train)}{AMPBConfig.COLOR_RESET} filas (hasta {y_train.index[-1].date()})\")\n",
    "print(f\"  Test:          {AMPBConfig.COLOR_VALUE}{len(y_test)}{AMPBConfig.COLOR_RESET} filas (desde {y_test.index[0].date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c58a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PROCESAR DATOS: TRANSFORMACIONES, ESCALADO, ALINEACION Y ANALISIS DE CALIDAD\n",
    "# Bajo determinadas circunstancias, puede abortar la ejecucion.\n",
    "processing_results = processData(\n",
    "    y_train, y_test, X_train, X_test,\n",
    "    y_train_original, y_test_original, X_train_original, X_test_original,\n",
    "    df_test, exog_vars, transformation, exog_scaling,\n",
    "    winsorization_value=winsorization_value,   \n",
    "    analyze=True               # True para ejecutar análisis de calidad\n",
    ")\n",
    "params_close = processing_results['params_close']\n",
    "params_exog  = processing_results['params_exog'] \n",
    "y_scaler = processing_results['y_scaler']\n",
    "exog_scaler = processing_results['exog_scaler']\n",
    "df_test_aligned = processing_results['df_test_aligned']\n",
    "prediction_max_limit = processing_results['prediction_max_limit']\n",
    "quality_results = processing_results['quality_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf095e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ENTRENAMIENTO LSTM (SOLO EN TRAIN DATA)\n",
    "\n",
    "# 4A. FUNCIONES PARA LSTM\n",
    "#  Versión optimizada del modelo LSTM\n",
    "def createModelLSTM(sequence_length, n_features, lstm_units, dropout_rate, learning_rate):\n",
    "    tf.keras.backend.clear_session()\n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    if len(lstm_units) == 1:\n",
    "        # 1 capa\n",
    "        model.add(LSTM(\n",
    "            lstm_units[0],\n",
    "            input_shape=(sequence_length, n_features),\n",
    "            return_sequences=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            kernel_regularizer=l2(2e-5),\n",
    "            dropout=0.0,             # sin dropout interno\n",
    "            recurrent_dropout=0.0\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate))  # único dropout (externo)\n",
    "\n",
    "    else:\n",
    "        # 2 capas\n",
    "        model.add(LSTM(\n",
    "            lstm_units[0],\n",
    "            input_shape=(sequence_length, n_features),\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            kernel_regularizer=l2(2e-5),\n",
    "            dropout=0.0,\n",
    "            recurrent_dropout=0.0\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(LSTM(\n",
    "            lstm_units[1],\n",
    "            return_sequences=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            kernel_regularizer=l2(2e-5),\n",
    "            dropout=0.0,\n",
    "            recurrent_dropout=0.0\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Salida directa\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=0.5, beta_1=0.9, beta_2=0.999, epsilon=1e-8\n",
    "        ),\n",
    "        loss='huber', # Huber es más robusto que MSE para outliers financieros (en este modelo al menos)\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Función objetivo optimizada con espacio de búsqueda más amplio y realista\n",
    "def objective(trial):    \n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "    # Espacio de búsqueda optimizado\n",
    "    sequence_length = trial.suggest_int('sequence_length', 10, 40, step=5)\n",
    "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n",
    "    lstm_units = []\n",
    "    for i in range(n_lstm_layers):\n",
    "        units = trial.suggest_int(f'lstm_units_l{i}', 96, 192, step=32) \n",
    "        lstm_units.append(units)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 96, 128]) \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.3, step=0.05) \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True) #v0\n",
    "    \n",
    "    # Split de validación\n",
    "    val_size = calculateValidationSplit(len(X_train), sequence_length, test_size)\n",
    "    if len(X_train) - val_size < sequence_length + 100:\n",
    "        raise optuna.TrialPruned(\"Datos insuficientes para esta configuración\")\n",
    "    \n",
    "    # Separar datos\n",
    "    X_train_optuna = X_train.values[:-val_size] \n",
    "    y_train_optuna = y_train[:-val_size]\n",
    "    X_val_optuna = X_train.values[-val_size:]   \n",
    "    y_val_optuna = y_train[-val_size:]\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = createTFDataset(\n",
    "        X_train_optuna, y_train_optuna, \n",
    "        sequence_length, batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = createTFDataset(\n",
    "        X_val_optuna, y_val_optuna, \n",
    "        sequence_length, batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = createModelLSTM(sequence_length, X_train.shape[1], lstm_units, dropout_rate, learning_rate)\n",
    "\n",
    "    # Callbacks optimizados\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=12,  \n",
    "            restore_best_weights=True, \n",
    "            min_delta=0.0001\n",
    "        ),\n",
    "        TFKerasPruningCallback(trial, 'val_loss'),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.6,  \n",
    "            patience=6, \n",
    "            min_lr=1e-7,        \n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "     \n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=35, \n",
    "        validation_data=val_dataset, \n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    \n",
    "    # Retornar mejor pérdida de validación\n",
    "    return min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4B. OPTIMIZACIÓN DE HIPERPARÁMETROS CON OPTUNA\n",
    "if optimize_hyperparams:\n",
    "    print(\"Buscando los mejores hiperparámetros para LSTM...\")\n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "\n",
    "    # Crear estudio Optuna con configuración optimizada\n",
    "    sampler = optuna.samplers.TPESampler(\n",
    "        seed=AMPBConfig.SEED,\n",
    "        n_startup_trials=10,\n",
    "        n_ei_candidates=24  \n",
    "    )\n",
    "    \n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "        min_resource=5,          # 1er corte en época 5\n",
    "        reduction_factor=3,      # pasa ~1/3 de trials\n",
    "        min_early_stopping_rate=0\n",
    "    )\n",
    "    study = optuna.create_study(\n",
    "         direction='minimize',\n",
    "         sampler=sampler,\n",
    "         pruner=pruner\n",
    "    )\n",
    "\n",
    "    # Callback para mostrar progreso\n",
    "    def print_progress(study, trial):\n",
    "        if trial.value is not None:\n",
    "            print(f\"{AMPBConfig.COLOR_MSG} Optimización LSTM: {len(study.trials)}/{n_optuna_trials} \"\n",
    "                  f\"(Val_Loss={trial.value:.6f}, Mejor={study.best_value:.6f}){AMPBConfig.COLOR_RESET}\")\n",
    "        else:\n",
    "            print(f\"{AMPBConfig.COLOR_MSG} Optimización LSTM: {len(study.trials)}/{n_optuna_trials} (Trial podado){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    # Ejecutar optimización\n",
    "    study.optimize(objective, n_trials=n_optuna_trials, callbacks=[print_progress])\n",
    "\n",
    "    # Resultados de Optuna    \n",
    "    if optuna_debug:\n",
    "        import optuna.visualization as vis    \n",
    "        fig_history = vis.plot_optimization_history(study) # Gráfico de evolución de la optimización\n",
    "        fig_history.show()\n",
    "        fig_importance = vis.plot_param_importances(study) # Importancia de hiperparámetros\n",
    "        fig_importance.show()\n",
    "        fig_contour = vis.plot_contour( # Relación entre parámetros (matriz de dispersión)\n",
    "            study, \n",
    "            params=[\"learning_rate\", \"lstm_units_l0\", \"dropout_rate\"]  # Parámetros clave\n",
    "        )\n",
    "        fig_contour.show()    \n",
    "        fig_slice = vis.plot_slice(study) # Distribución de los trials\n",
    "        fig_slice.show()\n",
    "    \n",
    "    # Obtener mejores hiperparámetros\n",
    "    best_params = study.best_params\n",
    "    n_layers = best_params['n_lstm_layers']\n",
    "    lstm_units = [best_params[f'lstm_units_l{i}'] for i in range(n_layers)]\n",
    "    \n",
    "    best_hyperparams = {\n",
    "        'sequence_length': best_params['sequence_length'],\n",
    "        'lstm_units': lstm_units,\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'learning_rate': best_params['learning_rate']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMejor Val_Loss: {study.best_value:.6f}\")\n",
    "    print(f\"Mejores hiperparámetros: {best_hyperparams}\")\n",
    "\n",
    "# 4C. USAR HIPERPARÁMETROS FIJOS\n",
    "else:   \n",
    "    best_hyperparams = {\n",
    "        'sequence_length': fixed_sequence_length,\n",
    "        'lstm_units': fixed_lstm_units,\n",
    "        'dropout_rate': fixed_dropout_rate,\n",
    "        'batch_size': fixed_batch_size,\n",
    "        'learning_rate': fixed_learning_rate\n",
    "    }\n",
    "    print(f\"Utilizando hiperparámetros fijos: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D. ENTRENAR MODELO FINAL\n",
    "print(\"Ajustando modelo...\")\n",
    "resetRandomSeeds(tensorflow_deterministic=True)\n",
    "\n",
    "# Crear split para validación del modelo final\n",
    "val_size = calculateValidationSplit(len(X_train), best_hyperparams['sequence_length'], test_size)\n",
    "X_train_final = X_train.values[:-val_size]\n",
    "y_train_final = y_train[:-val_size]\n",
    "X_val_final = X_train.values[-val_size:]\n",
    "y_val_final = y_train[-val_size:]\n",
    "\n",
    "# Crear datasets finales\n",
    "train_dataset = createTFDataset(\n",
    "    X_train_final, y_train_final, \n",
    "    best_hyperparams['sequence_length'], \n",
    "    best_hyperparams['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = createTFDataset(\n",
    "    X_val_final, y_val_final,\n",
    "    best_hyperparams['sequence_length'],\n",
    "    best_hyperparams['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Limpiar sesión y crear modelo final\n",
    "tf.keras.backend.clear_session() \n",
    "\n",
    "# Elegir arquitectura\n",
    "best_model = createModelLSTM(\n",
    "        best_hyperparams['sequence_length'],\n",
    "        X_train.shape[1],\n",
    "        best_hyperparams['lstm_units'],\n",
    "        best_hyperparams['dropout_rate'],\n",
    "        best_hyperparams['learning_rate']\n",
    ")\n",
    "\n",
    "# Callbacks para entrenamiento final y backtesting\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, min_delta=0.0001),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6), #v0\n",
    "    #ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7), # v3    \n",
    "]\n",
    "\n",
    "# Entrenar modelo final\n",
    "print(\"\", end=\"\")  # Para el progreso\n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=fixed_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1  # Mostrar progreso del entrenamiento final\n",
    ")\n",
    "\n",
    "# Crear título y hash del modelo\n",
    "hyperparams_str = f\"({best_hyperparams['sequence_length']},{best_hyperparams['lstm_units']},{best_hyperparams['dropout_rate']:.2f},{best_hyperparams['batch_size']},{best_hyperparams['learning_rate']:.5f})\"\n",
    "model_title, model_hash = createModelIdentity(model_name, model_version, hyperparams_str, transformation, exog_scaling, exog_set_id)\n",
    "\n",
    "print(f\"\\n{AMPBConfig.COLOR_INFO}Parámetros para {model_name}{model_version}:{AMPBConfig.COLOR_RESET}\")\n",
    "print(f\"Hiperparámetros utilizados:\")\n",
    "for param, value in best_hyperparams.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"Épocas entrenadas: {len(history.history['loss'])}\")\n",
    "print(f\"Val_Loss final: {min(history.history['val_loss']):.6f}\")\n",
    "print(f\"Título: '{model_title}' | HashID: {model_hash}\\n\")\n",
    "\n",
    "# Analizar los resultados de entrenamiento\n",
    "analyzeTrainingResults(history, best_model, X_val_final, y_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5A. PREDICCIÓN Y EVALUACIÓN EN EL CONJUNTO DE TEST (VALIDACIÓN ESTÁTICA)\n",
    "print(f\"{AMPBConfig.COLOR_INFO}Validación estática{AMPBConfig.COLOR_RESET}\")\n",
    "\n",
    "# Combinar datos transformados/escalados para crear secuencias\n",
    "full_X = np.vstack([X_train.values, X_test.values])\n",
    "full_y = np.concatenate([y_train.values, y_test.values])\n",
    "\n",
    "# Crear secuencias que incluyan el período de test\n",
    "X_full_seq, y_full_seq = createSequences(full_X, full_y, best_hyperparams['sequence_length'], include_current_day=True)\n",
    "\n",
    "# Encontrar el índice donde comienza el test en las secuencias\n",
    "test_start_idx = len(X_train) - best_hyperparams['sequence_length'] + 1\n",
    "X_test_seq = X_full_seq[test_start_idx:]\n",
    "\n",
    "# Realizar predicciones\n",
    "forecast_scaled_transformed = best_model.predict(X_test_seq, batch_size=best_hyperparams['batch_size'], verbose=0)\n",
    "forecast_scaled_transformed = forecast_scaled_transformed.flatten()\n",
    "\n",
    "# Ajustar índices - tomar solo los índices correspondientes al test\n",
    "available_test_indices = y_test.index[:len(forecast_scaled_transformed)]\n",
    "forecast_scaled_transformed = pd.Series(forecast_scaled_transformed, index=available_test_indices)\n",
    "\n",
    "# Aplicar pipeline de des-transformación\n",
    "forecast_original = reverseTransformPredictions(\n",
    "    forecast_scaled_transformed,\n",
    "    y_train_original.iloc[-1],\n",
    "    y_scaler,\n",
    "    transformation, \n",
    "    params_close,\n",
    "    prediction_max_limit\n",
    ")\n",
    "\n",
    "# Predicción día siguiente\n",
    "X_next_day = updateNextDayExog(\n",
    "    X_test,\n",
    "    feature_original_close=y_test_original.iloc[-1],          \n",
    "    transformation=transformation,\n",
    "    params_exog=params_exog,\n",
    "    exog_scaler=exog_scaler,\n",
    "    prev_open_original=(X_test_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    ")\n",
    "next_day_date = y_test.index[-1] + pd.tseries.offsets.BDay(1)\n",
    "\n",
    "# Tomar la última secuencia y añadir el nuevo día\n",
    "last_sequence = X_full_seq[-1]  # Última secuencia disponible\n",
    "    \n",
    "# Crear nueva secuencia: quitar el primer elemento y añadir las nuevas features\n",
    "next_sequence = np.vstack([last_sequence[1:], X_next_day.values])\n",
    "next_sequence = next_sequence.reshape(1, best_hyperparams['sequence_length'], X_train.shape[1])\n",
    "    \n",
    "next_forecast_scaled_transformed = best_model.predict(next_sequence, verbose=0).flatten()[0]\n",
    "next_forecast_scaled_transformed = pd.Series([next_forecast_scaled_transformed], index=[next_day_date])\n",
    "    \n",
    "next_day_forecast_original = reverseTransformPredictions(\n",
    "        next_forecast_scaled_transformed,\n",
    "        y_test_original.iloc[-1],  # Usar último valor real del test como referencia\n",
    "        y_scaler,\n",
    "        transformation,\n",
    "        params_close,\n",
    "        prediction_max_limit\n",
    ").iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed85c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5B. EVALUACIÓN Y GRÁFICAS\n",
    "y_test_original_values = y_test_original.iloc[:len(forecast_original)]\n",
    "    \n",
    "sv_r2, sv_mae, sv_rmse, sv_accuracy, sv_f1_score, sv_roc_auc = generateEvaluation(\n",
    "        y_test_original_values, \n",
    "        forecast_original, \n",
    "        df_test_aligned.iloc[:len(forecast_original)], \n",
    "        model_title, \n",
    "        model_hash, \n",
    "        next_day_date, \n",
    "        next_day_forecast_original, \n",
    "        \"Static Validation\"\n",
    ")\n",
    "            \n",
    "# Guardar reporte\n",
    "createReport(\n",
    "        model_name, \n",
    "        \"SV\", \n",
    "        f\"{transformation}_{exog_scaling}_{exog_set_id}\", \n",
    "        model_title, \n",
    "        model_hash, \n",
    "        sv_r2, \n",
    "        sv_mae, \n",
    "        sv_rmse, \n",
    "        sv_accuracy, \n",
    "        sv_f1_score, \n",
    "        sv_roc_auc\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43622c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EVALUACIÓN CON CROSS VALIDATION (VALIDACIÓN CRUZADA)\n",
    "if cv_final_evaluation:\n",
    "    from ampblib import createTimeSeriesCV, printMetricsCV\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Evaluación con Cross Validation (Validación cruzada){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    # Configuración CV\n",
    "    cv_folds = createTimeSeriesCV(X_train, y_train, n_splits=n_cv_folds, test_size=test_size, sequence_length=best_hyperparams['sequence_length'])\n",
    "    \n",
    "    if len(cv_folds) == 0:\n",
    "        print(f\"  Advertencia: No se pudieron crear folds suficientes para CV. Saltando...\")\n",
    "    else:\n",
    "        cv_results = []\n",
    "        cv_start_time = time.time()\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv_folds):\n",
    "            print(f\"   Fold {fold_idx + 1}/{len(cv_folds)}: \"\n",
    "                  f\"train={AMPBConfig.COLOR_VALUE}{len(train_idx)}{AMPBConfig.COLOR_RESET}, \"\n",
    "                  f\"val={AMPBConfig.COLOR_VALUE}{len(val_idx)}{AMPBConfig.COLOR_RESET}\", end='')\n",
    "            \n",
    "            # Preparar datos del fold\n",
    "            X_fold_train = X_train.values[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "            X_fold_val = X_train.values[val_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx]\n",
    "            y_fold_val_original = y_train_original.iloc[val_idx]\n",
    "            \n",
    "            # Validar que tenemos suficientes datos para secuencias\n",
    "            if len(X_fold_train) < best_hyperparams['sequence_length'] + 10:\n",
    "                print(f\" [Saltado: datos insuficientes]\")\n",
    "                continue\n",
    "            \n",
    "            # Crear datasets usando tu función existente\n",
    "            fold_train_dataset = createTFDataset(\n",
    "                X_fold_train, y_fold_train, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'], \n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            fold_val_dataset = createTFDataset(\n",
    "                X_fold_val, y_fold_val, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'], \n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Crear y entrenar modelo con los mejores hiperparámetros\n",
    "            tf.keras.backend.clear_session()\n",
    "            fold_model = createModelLSTM(\n",
    "                best_hyperparams['sequence_length'],\n",
    "                X_train.shape[1],\n",
    "                best_hyperparams['lstm_units'],\n",
    "                best_hyperparams['dropout_rate'],\n",
    "                best_hyperparams['learning_rate']\n",
    "            )           \n",
    "           \n",
    "            # Entrenar el modelo del fold\n",
    "            fold_history = fold_model.fit(\n",
    "                fold_train_dataset,\n",
    "                epochs=fixed_epochs,\n",
    "                validation_data=fold_val_dataset,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Hacer predicciones en el conjunto de validación del fold\n",
    "            fold_val_predictions_scaled = fold_model.predict(fold_val_dataset, verbose=0)\n",
    "            fold_val_predictions_scaled = fold_val_predictions_scaled.flatten()\n",
    "            \n",
    "            # Verificar que tenemos predicciones válidas\n",
    "            if len(fold_val_predictions_scaled) == 0:\n",
    "                print(f\" [Error: sin predicciones]\")\n",
    "                tf.keras.backend.clear_session()\n",
    "                del fold_model\n",
    "                continue\n",
    "            \n",
    "            # Crear series con índices apropiados para des-transformación\n",
    "            available_val_indices = y_fold_val_original.index[:len(fold_val_predictions_scaled)]\n",
    "            fold_predictions_scaled_series = pd.Series(fold_val_predictions_scaled, index=available_val_indices)\n",
    "            \n",
    "            # Des-transformar predicciones usando último valor de entrenamiento como referencia\n",
    "            reference_val = y_train_original.iloc[train_idx[-1]] if len(train_idx) > 0 else y_train_original.iloc[0]\n",
    "            \n",
    "            try:\n",
    "                fold_predictions_original = reverseTransformPredictions(\n",
    "                    fold_predictions_scaled_series,\n",
    "                    reference_val,\n",
    "                    y_scaler,\n",
    "                    transformation, \n",
    "                    params_close,\n",
    "                    prediction_max_limit\n",
    "                )\n",
    "                \n",
    "                # Obtener valores reales correspondientes\n",
    "                y_true_fold = y_fold_val_original.iloc[:len(fold_predictions_original)]\n",
    "                \n",
    "                # Calcular métricas de regresión\n",
    "                from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "                \n",
    "                fold_r2 = r2_score(y_true_fold, fold_predictions_original)\n",
    "                fold_mae = mean_absolute_error(y_true_fold, fold_predictions_original)\n",
    "                fold_rmse = np.sqrt(mean_squared_error(y_true_fold, fold_predictions_original))\n",
    "                \n",
    "                # MÉTRICAS DIRECCIONALES MEJORADAS\n",
    "                # Calcular dirección de movimiento (1 si sube, 0 si baja)\n",
    "                y_true_diff = y_true_fold.diff().dropna()\n",
    "                pred_diff = fold_predictions_original.diff().dropna()\n",
    "                \n",
    "                # Alinear índices en caso de diferencias\n",
    "                common_idx = y_true_diff.index.intersection(pred_diff.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    y_true_diff_aligned = y_true_diff.loc[common_idx]\n",
    "                    pred_diff_aligned = pred_diff.loc[common_idx]\n",
    "                    \n",
    "                    y_true_direction = (y_true_diff_aligned > 0).astype(int)\n",
    "                    pred_direction = (pred_diff_aligned > 0).astype(int)\n",
    "                    \n",
    "                    # Accuracy direccional\n",
    "                    if len(y_true_direction) > 0:\n",
    "                        fold_accuracy = (y_true_direction == pred_direction).mean()\n",
    "                        \n",
    "                        # F1-Score direccional\n",
    "                        from sklearn.metrics import f1_score, roc_auc_score\n",
    "                        try:\n",
    "                            fold_f1_score = f1_score(y_true_direction, pred_direction, zero_division=0)\n",
    "                        except:\n",
    "                            fold_f1_score = 0.0\n",
    "                        \n",
    "                        # ROC-AUC direccional (usando diferencias normalizadas como scores)\n",
    "                        try:\n",
    "                            if len(np.unique(y_true_direction)) > 1:  # Necesitamos ambas clases\n",
    "                                # Normalizar diferencias para usar como probabilidades\n",
    "                                pred_diff_norm = (pred_diff_aligned - pred_diff_aligned.min()) / (pred_diff_aligned.max() - pred_diff_aligned.min())\n",
    "                                pred_diff_norm = pred_diff_norm.fillna(0.5)\n",
    "                                fold_roc_auc = roc_auc_score(y_true_direction, pred_diff_norm)\n",
    "                            else:\n",
    "                                fold_roc_auc = 0.5  # Solo una clase presente\n",
    "                        except:\n",
    "                            fold_roc_auc = 0.5\n",
    "                    else:\n",
    "                        fold_accuracy = 0.0\n",
    "                        fold_f1_score = 0.0\n",
    "                        fold_roc_auc = 0.5\n",
    "                else:\n",
    "                    fold_accuracy = 0.0\n",
    "                    fold_f1_score = 0.0\n",
    "                    fold_roc_auc = 0.5\n",
    "                \n",
    "                # Guardar resultados del fold\n",
    "                epochs_trained = len(fold_history.history['loss'])\n",
    "                cv_results.append({\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'r2': fold_r2,\n",
    "                    'mae': fold_mae,\n",
    "                    'rmse': fold_rmse,\n",
    "                    'accuracy': fold_accuracy,\n",
    "                    'f1_score': fold_f1_score,\n",
    "                    'roc_auc': fold_roc_auc,\n",
    "                    'train_size': len(train_idx),\n",
    "                    'val_size': len(val_idx),\n",
    "                    'epochs_trained': epochs_trained,\n",
    "                    'final_val_loss': min(fold_history.history['val_loss'])\n",
    "                })\n",
    "                \n",
    "                # MOSTRAR RESULTADOS DEL FOLD CON MÁS INFORMACIÓN\n",
    "                print(f\" → R²={fold_r2:.4f}, MAE={fold_mae:.2f}, Acc={fold_accuracy:.3f}, F1={fold_f1_score:.3f}, Épocas={epochs_trained}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" [Error: {str(e)[:50]}...]\")\n",
    "            \n",
    "            # Limpiar memoria\n",
    "            tf.keras.backend.clear_session()\n",
    "            del fold_model\n",
    "        \n",
    "        cv_elapsed = time.time() - cv_start_time\n",
    "        \n",
    "        # USAR LA FUNCIÓN GENÉRICA PARA MOSTRAR ESTADÍSTICAS\n",
    "        cv_metrics = printMetricsCV(cv_results, cv_elapsed, len(cv_folds))\n",
    "\n",
    "else:\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Cross Validation deshabilitado{AMPBConfig.COLOR_RESET}\")\n",
    "    # Valores por defecto si CV está deshabilitado\n",
    "    cv_metrics = {\n",
    "        'cv_mean_r2': sv_r2,\n",
    "        'cv_mean_mae': sv_mae,\n",
    "        'cv_mean_rmse': sv_rmse,\n",
    "        'cv_mean_accuracy': sv_accuracy,\n",
    "        'cv_mean_f1_score': sv_f1_score,\n",
    "        'cv_mean_roc_auc': sv_roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7A. MODO DE BACKTESTING: PREDICCIÓN DÍA A DÍA (VALIDACIÓN BACKTESTING)\n",
    "run_backtesting = True\n",
    "if run_backtesting:    \n",
    "    model_title_backtest = f'{model_title} (Backtesting [{retrain_interval}d])'\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Modo Backtesting con Walk-Forward (Retrain cada {retrain_interval} días){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    # Inicializar historiales\n",
    "    history_y = y_train.copy()              # Datos transformados/escalados para el modelo\n",
    "    history_X = X_train.copy()              # Datos transformados/escalados para el modelo\n",
    "    history_y_original = y_train_original.copy()  # Valores originales para referencias\n",
    "    history_X_original = X_train_original.copy() \n",
    "    \n",
    "    predictions_original_bt = []\n",
    "    model_bt = None  # Modelo que se reutilizará entre reentrenamientos\n",
    "    \n",
    "    bt_start = time.time()\n",
    "    for t in range(len(y_test)):\n",
    "        print(f\" Backtesting: {t+1}/{len(y_test)}\", end='')\n",
    "        \n",
    "        # Reentrenar el modelo cuando sea necesario\n",
    "        if t % retrain_interval == 0:   \n",
    "            print(f\" [Reentrenando...]\", end='')\n",
    "            \n",
    "            val_size = calculateValidationSplit(len(history_y), best_hyperparams['sequence_length'], test_size)\n",
    "\n",
    "            X_bt_train = history_X.values[:-val_size]\n",
    "            y_bt_train = history_y[:-val_size]\n",
    "            X_bt_val = history_X.values[-val_size:]\n",
    "            y_bt_val = history_y[-val_size:]\n",
    "            \n",
    "            # Crear datasets\n",
    "            bt_train_dataset = createTFDataset(\n",
    "                X_bt_train, y_bt_train, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            bt_val_dataset = createTFDataset(\n",
    "                X_bt_val, y_bt_val,\n",
    "                best_hyperparams['sequence_length'],\n",
    "                best_hyperparams['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Limpiar sesión y crear modelo\n",
    "            tf.keras.backend.clear_session()            \n",
    "            model_bt = createModelLSTM(\n",
    "                best_hyperparams['sequence_length'],\n",
    "                X_train.shape[1],\n",
    "                best_hyperparams['lstm_units'],\n",
    "                best_hyperparams['dropout_rate'],\n",
    "                best_hyperparams['learning_rate']\n",
    "            )\n",
    "            \n",
    "            # Entranamiento\n",
    "            model_bt.fit(\n",
    "                bt_train_dataset,\n",
    "                epochs=fixed_epochs,\n",
    "                validation_data=bt_val_dataset,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            print(f\" [✓]\", end='')\n",
    "        \n",
    "        print(f\"\")\n",
    "        \n",
    "        # Generar variables exógenas propagadas para el día t que vamos a predecir\n",
    "        X_current_propagated = updateNextDayExog(\n",
    "            history_X,\n",
    "            feature_original_close=history_y_original.iloc[-1],        # Close_{t-1} original\n",
    "            transformation=transformation,\n",
    "            params_exog=params_exog,\n",
    "            exog_scaler=exog_scaler,\n",
    "            prev_open_original=(history_X_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    "        )\n",
    "        \n",
    "        # Combinar datos históricos + variables exógenas propagadas para crear secuencias. Esto es necesario porque LSTM necesita secuencias completas hasta el día t\n",
    "        current_full_X = np.vstack([history_X.values, X_current_propagated.values])\n",
    "        current_full_y = np.concatenate([history_y.values, [history_y.iloc[-1]]]) \n",
    "        \n",
    "        # Crear secuencias con datos históricos + variables exógenas propagadas\n",
    "        X_current_seq, y_current_seq = createSequences(current_full_X, current_full_y, best_hyperparams['sequence_length'], include_current_day=False)\n",
    "        \n",
    "        # Verificar que tenemos suficientes secuencias\n",
    "        if len(X_current_seq) == 0:\n",
    "            print(f\" [Error: No hay suficientes datos para crear secuencias en t={t}]\")\n",
    "            continue\n",
    "        \n",
    "        # Tomar la última secuencia disponible (que incluye las variables exógenas propagadas)\n",
    "        X_prediction_seq = X_current_seq[-1:]\n",
    "        \n",
    "        # Predecir \n",
    "        forecast_step_scaled_transformed = model_bt.predict(X_prediction_seq, batch_size=best_hyperparams['batch_size'], verbose=0)\n",
    "        forecast_step_scaled_transformed = forecast_step_scaled_transformed.flatten()[0]\n",
    "        forecast_step_scaled_transformed = pd.Series([forecast_step_scaled_transformed], index=[y_test.index[t]])\n",
    "        \n",
    "        # Des-transformar usando la última referencia disponible\n",
    "        reference_val = history_y_original.iloc[-1]\n",
    "        forecast_step_original = reverseTransformPredictions(\n",
    "            forecast_step_scaled_transformed,\n",
    "            reference_val,\n",
    "            y_scaler,\n",
    "            transformation,\n",
    "            params_close,\n",
    "            prediction_max_limit\n",
    "        ).iloc[0]\n",
    "        \n",
    "        # Guardar predicción\n",
    "        predictions_original_bt.append(forecast_step_original)\n",
    "        \n",
    "        # Actualizar historiales con datos reales del día t\n",
    "        history_y = pd.concat([history_y, y_test.iloc[t:t+1]])\n",
    "        history_X = pd.concat([history_X, X_test.iloc[t:t+1]])\n",
    "        history_y_original = pd.concat([history_y_original, pd.Series([y_test_original.iloc[t]], index=[y_test_original.index[t]])])\n",
    "        history_X_original = pd.concat([history_X_original, X_test_original.iloc[t:t+1]])\n",
    "    \n",
    "    # Estadísticas de reentrenamiento\n",
    "    total_retrains = (len(y_test) + retrain_interval - 1) // retrain_interval  # Ceiling division\n",
    "    print(f\" Backtesting completado en {time.time() - bt_start:.1f}s\")\n",
    "    print(f\" Reentrenamientos realizados: {total_retrains} (cada {retrain_interval} días)\")\n",
    "    \n",
    "    # Crear Serie con predicciones del backtesting\n",
    "    forecast_backtest_original = pd.Series(predictions_original_bt, index=y_test_original.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d04332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7B. MODO DE BACKTESTING: PREDICCIÓN SIGUIENTE DÍA  \n",
    "if run_backtesting:\n",
    "    # Usar updateNextDayExogNew() para el día siguiente  \n",
    "    X_next_day_bt = updateNextDayExog(\n",
    "        history_X,\n",
    "        feature_original_close=history_y_original.iloc[-1],\n",
    "        transformation=transformation,\n",
    "        params_exog=params_exog,\n",
    "        exog_scaler=exog_scaler,\n",
    "        prev_open_original=(history_X_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    "    )\n",
    "\n",
    "    next_day_date = y_test.index[-1] + pd.tseries.offsets.BDay(1)\n",
    "    X_next_day_bt.index = [next_day_date]\n",
    "    \n",
    "    # Crear secuencias finales para predicción del día siguiente\n",
    "    final_full_X = np.vstack([history_X.values, X_next_day_bt.values])\n",
    "    final_full_y = np.concatenate([history_y.values, [history_y.iloc[-1]]])  # Dummy para createSequences\n",
    "    \n",
    "    X_final_seq, y_final_seq = createSequences(final_full_X, final_full_y, best_hyperparams['sequence_length'], include_current_day=False)\n",
    "    \n",
    "    # Tomar la última secuencia\n",
    "    last_sequence = X_final_seq[-1:]\n",
    "    \n",
    "    # Predecir día siguiente\n",
    "    next_forecast_scaled_transformed_bt = model_bt.predict(last_sequence, verbose=0).flatten()[0]\n",
    "    next_forecast_scaled_transformed_bt = pd.Series([next_forecast_scaled_transformed_bt], index=[next_day_date])\n",
    "    \n",
    "    # Des-transformar usando la última referencia del backtesting\n",
    "    next_day_forecast_val_bt_original = reverseTransformPredictions(\n",
    "        next_forecast_scaled_transformed_bt, \n",
    "        history_y_original.iloc[-1],   # Última referencia conocida en backtesting\n",
    "        y_scaler, \n",
    "        transformation, \n",
    "        params_close,\n",
    "        prediction_max_limit\n",
    "    ).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7C. MODO DE BACKTESTING: EVALUACIÓN Y GRÁFICAS\n",
    "if run_backtesting:\n",
    "    # Evaluación del backtesting \n",
    "    bt_r2, bt_mae, bt_rmse, bt_accuracy, bt_f1_score, bt_roc_auc = generateEvaluation(\n",
    "        y_test_original, \n",
    "        forecast_backtest_original, \n",
    "        df_test_aligned, \n",
    "        model_title_backtest, \n",
    "        model_hash,\n",
    "        next_day_date, \n",
    "        next_day_forecast_val_bt_original, \n",
    "        \"Backtesting\"\n",
    "    )\n",
    "\n",
    "    # Guardar informe del backtesting \n",
    "    createReport(\n",
    "        model_name, \n",
    "        \"BT\", \n",
    "        f\"{transformation}_{exog_scaling}_{exog_set_id}\", \n",
    "        model_title_backtest, \n",
    "        model_hash, \n",
    "        bt_r2, \n",
    "        bt_mae, \n",
    "        bt_rmse, \n",
    "        bt_accuracy, \n",
    "        bt_f1_score, \n",
    "        bt_roc_auc\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f2c10a",
   "metadata": {},
   "source": [
    "<div style=\"width:90%; margin:0 auto; background-color:#F0F0F0; padding:20px; border-radius:8px; font-family:Arial, sans-serif\">\n",
    "\n",
    "<div style=\"display:flex; align-items:center; justify-content:space-between; margin-bottom:15px\">\n",
    "    <div style=\"width:100px; height:100px\">\n",
    "        <img src=\"data:image/png;base64,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\" style=\"max-width:100%; max-height:100%\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center; margin-bottom:20px\">\n",
    "        <h1 style=\"color:#9E0B0F; font-size:26px; margin-bottom:5px\">Análisis de modelos predictivos en bolsa</h1>\n",
    "        <h3 style=\"color:#D64550; font-size:22px; margin-top:0\">Copyright (C) 2024-2025 MegaStorm Systems</h3>\n",
    "    </div>\n",
    "    <div style=\"width:192px; height:59px\">\n",
    "        <p style=\"max-width:100%; max-height:100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:60%; margin:0 auto; background-color:#E8E8E8; padding:20px; border-radius:10px; border:1px solid #D0D0D0; margin-bottom:20px; color:#333333; line-height:1.5; box-shadow:0 2px 4px rgba(0,0,0,0.05)\">\n",
    "    <p style=\"margin:0; font-size:15px\">    \n",
    "    This software is provided \"as-is\", without any express or implied<br>\n",
    "    warranty. In no event will the authors be held liable for any damages<br>\n",
    "    arising from the use of this software.<br><br>\n",
    "    Permission is granted to anyone to use this software for any purpose,<br>\n",
    "    including commercial applications, and to alter it and redistribute it<br>\n",
    "    freely, subject to the following restrictions:<br><br>\n",
    "    1. The origin of this software must not be misrepresented; you must not<br>\n",
    "    claim that you wrote the original software. If you use this software<br>\n",
    "    in a product, an acknowledgment in the product documentation would be<br>\n",
    "    appreciated but is not required.<br>\n",
    "    2. Altered source versions must be plainly marked as such, and must not be<br>\n",
    "    misrepresented as being the original software.<br>\n",
    "    3. This notice may not be removed or altered from any source distribution.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <h2 style=\"font-size:24px; color:#9E0B0F; margin:0\">Predictor Transformer v3.4</h2>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7b7ad-0674-4820-9e5a-616fb930fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "import argparse\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ampblib import AMPBConfig, processData, generateEvaluation, updateNextDayExog, createReport, getExogVars, reverseTransformPredictions, createModelIdentity, createTFDataset, analyzeTrainingResults, createSequences, setupTensorflowDevice, resetRandomSeeds, calculateValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. PARÁMETROS CONFIGURABLES\n",
    "model_name = \"Transformer\"\n",
    "model_version = \"v3.4\"\n",
    "\n",
    "# Por defecto, permite ejecucion interactiva\n",
    "default_transformation = \"YeoJohnson\" # \"Log\", \"YeoJohnson\"\n",
    "default_exog_scaling = \"Standard\"   # \"Standard\"\n",
    "default_exog_set_id = 123456    # 1=\"Directos\", 2=\"IndicadoresTecnicos\", 3=\"BigTech\", 4=\"IndicesBursatiles\", 5=\"IndicadoresEconomicos\", 6=\"AnalisisSentimiento\"\n",
    "\n",
    "# Estos son fijos e internos, no los exponemos\n",
    "nombre_archivo = \"NVDA_2015-01-05_2025-05-23_SA.csv\"\n",
    "test_size = 60              # Número de días para el conjunto de test\n",
    "optimize_hyperparams = True # True para optimizar hiperparámetros con Optuna\n",
    "winsorization_value = 0.05  # Aplicar winsorización. Este valor es el mejor encontrado, 0.04 y 0.06 son peores y a mayor valor, mas valores que modidicamos.\n",
    "run_backtesting = True      # True para Backtesting con Walk-Forward\n",
    "fixed_epochs = 40\n",
    "retrain_interval = 5        # Reentrenar modelo completo cada n días en backtesting. 5 es un valor adecuado, acelera la velocidad a cambio de perder un ~6% de rendimiento.\n",
    "n_optuna_trials = 10        # Número de trials para Optuna\n",
    "optuna_debug = False\n",
    "cv_final_evaluation = False  # Cambiar a False para deshabilitar cross-validation\n",
    "n_cv_folds = 5              # Separación de unos 8 meses\n",
    "\n",
    "# Configuración determinista de TensorFlow\n",
    "resetRandomSeeds(tensorflow_deterministic=True)\n",
    "setupTensorflowDevice(use_gpu=False)\n",
    "\n",
    "# Hiperparámetros fijos si optimize_hyperparams = False\n",
    "fixed_sequence_length = 10\n",
    "fixed_d_model = 128\n",
    "fixed_num_heads = 4\n",
    "fixed_num_transformer_blocks = 1\n",
    "fixed_dropout_rate = 0.15\n",
    "fixed_batch_size = 128\n",
    "fixed_learning_rate = 0.000747599299995651\n",
    "fixed_ff_dim = 128\n",
    "\n",
    "# Argumentos de línea de comandos  \n",
    "if AMPBConfig.INTERACTIVE:\n",
    "    transformation = default_transformation\n",
    "    exog_scaling = default_exog_scaling\n",
    "    exog_set_id = default_exog_set_id\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description='Ejecuta modelo LSTM.')\n",
    "    parser.add_argument('--transformation', type=str, default=default_transformation, choices=['None', 'Log', 'RetLog', 'YeoJohnson'])\n",
    "    parser.add_argument('--exog_scaling', type=str, default=default_exog_scaling, choices=['None', 'Standard', 'MinMax'])\n",
    "    def valid_exog(x):\n",
    "        if not all(c in '123456' for c in str(x)): \n",
    "            raise argparse.ArgumentTypeError(f\"Solo dígitos 1-6: {x}\")\n",
    "        return x\n",
    "    parser.add_argument('--exog_set_id', type=valid_exog, default=default_exog_set_id)\n",
    "    args = parser.parse_args()\n",
    "    transformation = args.transformation\n",
    "    exog_scaling = args.exog_scaling\n",
    "    exog_set_id = args.exog_set_id\n",
    "\n",
    "AMPBConfig.printHeader(title=f\"Predictor {model_name} {model_version}\", testsize=test_size, \n",
    "                     optimize=optimize_hyperparams, backtesting=run_backtesting, transform=transformation,\n",
    "                     exogscaling=exog_scaling, exogsetid=exog_set_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGA Y PREPARACIÓN DE DATOS\n",
    "datos = pd.read_csv(nombre_archivo)\n",
    "\n",
    "# Seleccionar columnas relevantes\n",
    "mandatory_vars = ['Date','Close', 'Trend']\n",
    "\n",
    "# Obtenemos lista de exogenas\n",
    "exog_vars = getExogVars(exog_set_id)\n",
    "df = datos[mandatory_vars + exog_vars]\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])  \n",
    "\n",
    "# Verificar y mostrar estadísticas básicas de los datos\n",
    "print(f\"Datos en crudo cargados: {len(df)} registros de {df['Date'].min()} a {df['Date'].max()}.\")\n",
    "\n",
    "# Detectar valores faltantes\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "if not missing.empty:\n",
    "    total_before = len(df)\n",
    "    print(\"Valores faltantes por columna (se borrarán estas filas):\")\n",
    "    for col, cnt in missing.items():\n",
    "        print(f\"  • {col}: {cnt} valores faltantes\")\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    total_after = len(df)\n",
    "    removed = total_before - total_after\n",
    "    print(f\"\\nFilas borradas: {removed}\")\n",
    "else:\n",
    "    print(\"No se encontraron valores faltantes.\")\n",
    "\n",
    "# Poner Date como indice\n",
    "df.set_index('Date', inplace=True)\n",
    "#df = df.asfreq('B', method='pad')  # Se asume que se trata de datos bursátiles (días hábiles)\n",
    "\n",
    "print(f\"\\nDatos cargados: {AMPBConfig.COLOR_VALUE}{len(df)}{AMPBConfig.COLOR_RESET} registros. Variables exógenas seleccionadas: {AMPBConfig.COLOR_VALUE}{len(exog_vars)}{AMPBConfig.COLOR_RESET}\\n{exog_vars}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DIVISIÓN ENTRE ENTRENAMIENTO Y TEST\n",
    "\n",
    "# 2A. Se utiliza el 90% de los datos para entrenamiento y el 10% para test\n",
    "#split_index = int(len(df) * 0.90)\n",
    "#df_train = df.iloc[:split_index].copy()\n",
    "#df_test = df.iloc[split_index:].copy()\n",
    "\n",
    "# 2B. Separamos datos de entrenamiento y de test por fecha\n",
    "#split_date = pd.Timestamp('2024-12-01')\n",
    "#df_train = df.loc[:split_date].copy()\n",
    "#df_test = df.loc[split_date:].copy()\n",
    "\n",
    "# 2C. Separamos por numero de dias.\n",
    "df_train = df.iloc[:-test_size]\n",
    "df_test = df.iloc[-test_size:]\n",
    "\n",
    "# Guardar valores originales (inmutables para referencia futura)\n",
    "y_train_original = df_train['Close'].copy()\n",
    "y_test_original = df_test['Close'].copy()\n",
    "X_train_original = df_train[exog_vars].copy()  \n",
    "X_test_original = df_test[exog_vars].copy()\n",
    "\n",
    "# Variables de trabajo (se transformarán/escalarán según configuración)\n",
    "y_train = df_train['Close']\n",
    "y_test  = df_test['Close']\n",
    "X_train = df_train[exog_vars].copy()\n",
    "X_test = df_test[exog_vars].copy()\n",
    "\n",
    "print(f\"\\nDatos divididos:\")\n",
    "print(f\"  Entrenamiento: {AMPBConfig.COLOR_VALUE}{len(y_train)}{AMPBConfig.COLOR_RESET} filas (hasta {y_train.index[-1].date()})\")\n",
    "print(f\"  Test:          {AMPBConfig.COLOR_VALUE}{len(y_test)}{AMPBConfig.COLOR_RESET} filas (desde {y_test.index[0].date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c58a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PROCESAR DATOS: TRANSFORMACIONES, ESCALADO, ALINEACION Y ANALISIS DE CALIDAD\n",
    "# Bajo determinadas circunstancias, puede abortar la ejecucion.\n",
    "processing_results = processData(\n",
    "    y_train, y_test, X_train, X_test,\n",
    "    y_train_original, y_test_original, X_train_original, X_test_original,\n",
    "    df_test, exog_vars, transformation, exog_scaling,\n",
    "    winsorization_value=winsorization_value,   \n",
    "    analyze=True               # True para ejecutar análisis de calidad\n",
    ")\n",
    "params_close = processing_results['params_close']\n",
    "params_exog  = processing_results['params_exog'] \n",
    "y_scaler = processing_results['y_scaler']\n",
    "exog_scaler = processing_results['exog_scaler']\n",
    "df_test_aligned = processing_results['df_test_aligned']\n",
    "prediction_max_limit = processing_results['prediction_max_limit']\n",
    "quality_results = processing_results['quality_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf095e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ENTRENAMIENTO TRANSFORMER (SOLO EN TRAIN DATA)\n",
    "\n",
    "# 4A. FUNCIONES PARA TRANSFORMER\n",
    "\n",
    "#  Función para construir el modelo Transformer Encoder\n",
    "def createModelTransformer(sequence_length, n_features, d_model, num_heads, ff_dim, num_transformer_blocks, dropout_rate, learning_rate):\n",
    "    tf.keras.backend.clear_session()\n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
    "    \n",
    "    # 1. Proyección de features: Convierte el vector de n_features a la dimensión interna d_model\n",
    "    # Se usa TimeDistributed para aplicar la misma capa Dense a cada paso de tiempo de la secuencia\n",
    "    x = TimeDistributed(Dense(d_model, kernel_regularizer=l2(1e-5)))(inputs)\n",
    "\n",
    "    # 2. Codificación posicional: para secuencias cortas, la red puede aprender posiciones relativas sin codificación explícita.\n",
    "\n",
    "    # 3. Bloques Transformer Encoder\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Subcapa de Atención Multi-Cabeza\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(x, x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output) # Conexión residual y normalización\n",
    "\n",
    "        # Subcapa de Red Feed-Forward\n",
    "        ffn_output = Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\", kernel_regularizer=l2(1e-5)),\n",
    "            Dense(d_model, kernel_regularizer=l2(1e-5))\n",
    "        ])(x)\n",
    "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ffn_output) # Conexión residual y normalización\n",
    "    \n",
    "    # 4. Pooling y Cabezal de Salida\n",
    "    # Agrega la información de toda la secuencia en un único vector\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x) # Capa densa intermedia\n",
    "    outputs = Dense(1, activation=\"linear\")(x) # Salida de regresión\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=0.7, beta_1=0.9, beta_2=0.999\n",
    "        ),\n",
    "        loss='mae', # MAE se comporta mejor que huber (sobre todo en backtesting!)\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Función objetivo para Optuna con hiperparámetros de Transformer\n",
    "def objective(trial):    \n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "    \n",
    "    # Espacio de búsqueda para Transformer\n",
    "    sequence_length = trial.suggest_int('sequence_length', 10, 30, step=5)\n",
    "    d_model = trial.suggest_categorical('d_model', [64, 96, 128])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [4, 8])\n",
    "    ff_dim = trial.suggest_categorical('ff_dim', [128, 256])\n",
    "    num_transformer_blocks = trial.suggest_int('num_transformer_blocks', 1, 2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 96, 128]) \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.35, step=0.05) \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "    \n",
    "    # Asegurar que d_model es divisible por num_heads\n",
    "    if d_model % num_heads != 0:\n",
    "        raise optuna.TrialPruned(\"d_model debe ser divisible por num_heads.\")\n",
    "\n",
    "    # Split de validación (la misma lógica que antes)\n",
    "    val_size = calculateValidationSplit(len(X_train), sequence_length, test_size)\n",
    "    if len(X_train) - val_size < sequence_length + 100:\n",
    "        raise optuna.TrialPruned(\"Datos insuficientes para esta configuración\")\n",
    "    \n",
    "    X_train_optuna, y_train_optuna = X_train.values[:-val_size], y_train[:-val_size]\n",
    "    X_val_optuna, y_val_optuna = X_train.values[-val_size:], y_train[-val_size:]\n",
    "    \n",
    "    train_dataset = createTFDataset(X_train_optuna, y_train_optuna, sequence_length, batch_size, shuffle=True)\n",
    "    val_dataset = createTFDataset(X_val_optuna, y_val_optuna, sequence_length, batch_size, shuffle=False)\n",
    "    \n",
    "    model = createModelTransformer(\n",
    "        sequence_length, X_train.shape[1], \n",
    "        d_model, num_heads, ff_dim, \n",
    "        num_transformer_blocks, dropout_rate, learning_rate\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=0.0001),\n",
    "        TFKerasPruningCallback(trial, 'val_loss'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=6, min_lr=1e-7, verbose=0)\n",
    "    ]\n",
    "     \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=40, # Aumentamos ligeramente para dar más margen al Transformer\n",
    "        validation_data=val_dataset, \n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    return min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4B. OPTIMIZACIÓN DE HIPERPARÁMETROS CON OPTUNA\n",
    "if optimize_hyperparams:\n",
    "    print(\"Buscando los mejores hiperparámetros para Transformer...\")\n",
    "    resetRandomSeeds(tensorflow_deterministic=True)\n",
    "    \n",
    "    # La configuración de Optuna (sampler, pruner) es reutilizable\n",
    "    sampler = optuna.samplers.TPESampler(seed=AMPBConfig.SEED, n_startup_trials=10, n_ei_candidates=24)\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource=5, reduction_factor=3, min_early_stopping_rate=0)\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
    "\n",
    "    def print_progress(study, trial):\n",
    "        if trial.value is not None:\n",
    "            print(f\"{AMPBConfig.COLOR_MSG} Optimización Transformer: {len(study.trials)}/{n_optuna_trials} \"\n",
    "                  f\"(Val_Loss={trial.value:.6f}, Mejor={study.best_value:.6f}){AMPBConfig.COLOR_RESET}\")\n",
    "        else:\n",
    "            print(f\"{AMPBConfig.COLOR_MSG} Optimización Transformer: {len(study.trials)}/{n_optuna_trials} (Trial podado){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_optuna_trials, callbacks=[print_progress])\n",
    "    \n",
    "    # El código de visualización de Optuna sigue siendo válido\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_hyperparams = {\n",
    "        'sequence_length': best_params['sequence_length'],\n",
    "        'd_model': best_params['d_model'],\n",
    "        'num_heads': best_params['num_heads'],        \n",
    "        'num_transformer_blocks': best_params['num_transformer_blocks'],\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'ff_dim': best_params['ff_dim']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMejor Val_Loss: {study.best_value:.6f}\")\n",
    "    print(f\"Mejores hiperparámetros: {best_hyperparams}\")\n",
    "\n",
    "# 4C. USAR HIPERPARÁMETROS FIJOS\n",
    "else:   \n",
    "    # Actualizar con hiperparámetros fijos para el Transformer\n",
    "    best_hyperparams = {\n",
    "        'sequence_length': fixed_sequence_length,\n",
    "        'batch_size': fixed_batch_size,\n",
    "        'd_model': fixed_d_model,   \n",
    "        'num_heads': fixed_num_heads,        \n",
    "        'num_transformer_blocks': fixed_num_transformer_blocks,\n",
    "        'dropout_rate': fixed_dropout_rate,\n",
    "        'learning_rate': fixed_learning_rate,\n",
    "        'ff_dim': fixed_ff_dim\n",
    "    }\n",
    "    print(f\"Utilizando hiperparámetros fijos: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D. ENTRENAR MODELO FINAL\n",
    "print(\"Ajustando modelo final Transformer...\")\n",
    "resetRandomSeeds(tensorflow_deterministic=True)\n",
    "\n",
    "# La lógica de creación de datasets finales es la misma\n",
    "val_size = calculateValidationSplit(len(X_train), best_hyperparams['sequence_length'], test_size)\n",
    "X_train_final, y_train_final = X_train.values[:-val_size], y_train[:-val_size]\n",
    "X_val_final, y_val_final = X_train.values[-val_size:], y_train[-val_size:]\n",
    "\n",
    "train_dataset = createTFDataset(\n",
    "    X_train_final, y_train_final, \n",
    "    best_hyperparams['sequence_length'], \n",
    "    best_hyperparams['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_dataset = createTFDataset(\n",
    "    X_val_final, y_val_final,\n",
    "    best_hyperparams['sequence_length'],\n",
    "    best_hyperparams['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session() \n",
    "\n",
    "# Llamar a la nueva función de creación de modelo\n",
    "best_model = createModelTransformer(\n",
    "    best_hyperparams['sequence_length'],\n",
    "    X_train.shape[1],\n",
    "    best_hyperparams['d_model'],\n",
    "    best_hyperparams['num_heads'],\n",
    "    best_hyperparams['ff_dim'],\n",
    "    best_hyperparams['num_transformer_blocks'],\n",
    "    best_hyperparams['dropout_rate'],\n",
    "    best_hyperparams['learning_rate']\n",
    ")\n",
    "\n",
    "# Los callbacks para el entrenamiento final son los mismos\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, min_delta=0.0001),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6),\n",
    "]\n",
    "\n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=fixed_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Actualizar el string de hiperparámetros para el reporte\n",
    "hyperparams_str = f\"({best_hyperparams['sequence_length']},{best_hyperparams['d_model']},{best_hyperparams['num_heads']},{best_hyperparams['num_transformer_blocks']},{best_hyperparams['dropout_rate']:.2f},{best_hyperparams['batch_size']},{best_hyperparams['learning_rate']:.5f})\"\n",
    "model_title, model_hash = createModelIdentity(model_name, model_version, hyperparams_str, transformation, exog_scaling, exog_set_id)\n",
    "\n",
    "print(f\"\\n{AMPBConfig.COLOR_INFO}Parámetros para {model_name}{model_version}:{AMPBConfig.COLOR_RESET}\")\n",
    "print(f\"Hiperparámetros utilizados:\")\n",
    "for param, value in best_hyperparams.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"Épocas entrenadas: {len(history.history['loss'])}\")\n",
    "print(f\"Val_Loss final: {min(history.history['val_loss']):.6f}\")\n",
    "print(f\"Título: '{model_title}' | HashID: {model_hash}\\n\")\n",
    "\n",
    "analyzeTrainingResults(history, best_model, X_val_final, y_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5A. PREDICCIÓN Y EVALUACIÓN EN EL CONJUNTO DE TEST (VALIDACIÓN ESTÁTICA)\n",
    "print(f\"{AMPBConfig.COLOR_INFO}Validación Estática{AMPBConfig.COLOR_RESET}\")\n",
    "\n",
    "# Combinar datos transformados/escalados para crear secuencias\n",
    "full_X = np.vstack([X_train.values, X_test.values])\n",
    "full_y = np.concatenate([y_train.values, y_test.values])\n",
    "\n",
    "# Crear secuencias que incluyan el período de test\n",
    "X_full_seq, y_full_seq = createSequences(full_X, full_y, best_hyperparams['sequence_length'], include_current_day=True)\n",
    "\n",
    "# Encontrar el índice donde comienza el test en las secuencias\n",
    "test_start_idx = len(X_train) - best_hyperparams['sequence_length'] + 1\n",
    "X_test_seq = X_full_seq[test_start_idx:]\n",
    "\n",
    "# Realizar predicciones\n",
    "forecast_scaled_transformed = best_model.predict(X_test_seq, batch_size=best_hyperparams['batch_size'], verbose=0)\n",
    "forecast_scaled_transformed = forecast_scaled_transformed.flatten()\n",
    "\n",
    "# Ajustar índices - tomar solo los índices correspondientes al test\n",
    "available_test_indices = y_test.index[:len(forecast_scaled_transformed)]\n",
    "forecast_scaled_transformed = pd.Series(forecast_scaled_transformed, index=available_test_indices)\n",
    "\n",
    "# Aplicar pipeline de des-transformación\n",
    "forecast_original = reverseTransformPredictions(\n",
    "    forecast_scaled_transformed,\n",
    "    y_train_original.iloc[-1],\n",
    "    y_scaler,\n",
    "    transformation, \n",
    "    params_close,\n",
    "    prediction_max_limit\n",
    ")\n",
    "\n",
    "# Predicción día siguiente\n",
    "X_next_day = updateNextDayExog(\n",
    "    X_test,\n",
    "    feature_original_close=y_test_original.iloc[-1],          \n",
    "    transformation=transformation,\n",
    "    params_exog=params_exog,\n",
    "    exog_scaler=exog_scaler,\n",
    "    prev_open_original=(X_test_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    ")\n",
    "next_day_date = y_test.index[-1] + pd.tseries.offsets.BDay(1)\n",
    "\n",
    "# Tomar la última secuencia y añadir el nuevo día\n",
    "last_sequence = X_full_seq[-1]  # Última secuencia disponible\n",
    "    \n",
    "# Crear nueva secuencia: quitar el primer elemento y añadir las nuevas features\n",
    "next_sequence = np.vstack([last_sequence[1:], X_next_day.values])\n",
    "next_sequence = next_sequence.reshape(1, best_hyperparams['sequence_length'], X_train.shape[1])\n",
    "    \n",
    "next_forecast_scaled_transformed = best_model.predict(next_sequence, verbose=0).flatten()[0]\n",
    "next_forecast_scaled_transformed = pd.Series([next_forecast_scaled_transformed], index=[next_day_date])\n",
    "    \n",
    "next_day_forecast_original = reverseTransformPredictions(\n",
    "        next_forecast_scaled_transformed,\n",
    "        y_test_original.iloc[-1],  # Usar último valor real del test como referencia\n",
    "        y_scaler,\n",
    "        transformation,\n",
    "        params_close,\n",
    "        prediction_max_limit\n",
    ").iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed85c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5B. EVALUACIÓN Y GRÁFICAS\n",
    "y_test_original_values = y_test_original.iloc[:len(forecast_original)]\n",
    "    \n",
    "sv_r2, sv_mae, sv_rmse, sv_accuracy, sv_f1_score, sv_roc_auc = generateEvaluation(\n",
    "        y_test_original_values, \n",
    "        forecast_original, \n",
    "        df_test_aligned.iloc[:len(forecast_original)], \n",
    "        model_title, \n",
    "        model_hash, \n",
    "        next_day_date, \n",
    "        next_day_forecast_original, \n",
    "        \"Static Validation\"\n",
    ")\n",
    "            \n",
    "# Guardar reporte\n",
    "createReport(\n",
    "        model_name, \n",
    "        \"SV\", \n",
    "        f\"{transformation}_{exog_scaling}_{exog_set_id}\", \n",
    "        model_title, \n",
    "        model_hash, \n",
    "        sv_r2, \n",
    "        sv_mae, \n",
    "        sv_rmse, \n",
    "        sv_accuracy, \n",
    "        sv_f1_score, \n",
    "        sv_roc_auc\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad669de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EVALUACIÓN CON CROSS VALIDATION (VALIDACIÓN CRUZADA)\n",
    "if cv_final_evaluation:\n",
    "    from ampblib import createTimeSeriesCV, printMetricsCV\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Evaluación con Cross Validation (Validación cruzada){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    # Configuración CV\n",
    "    cv_folds = createTimeSeriesCV(X_train, y_train, n_splits=n_cv_folds, test_size=test_size, sequence_length=best_hyperparams['sequence_length'])\n",
    "    \n",
    "    if len(cv_folds) == 0:\n",
    "        print(f\"  Advertencia: No se pudieron crear folds suficientes para CV. Saltando...\")\n",
    "    else:\n",
    "        cv_results = []\n",
    "        cv_start_time = time.time()\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv_folds):\n",
    "            print(f\"   Fold {fold_idx + 1}/{len(cv_folds)}: \"\n",
    "                  f\"train={AMPBConfig.COLOR_VALUE}{len(train_idx)}{AMPBConfig.COLOR_RESET}, \"\n",
    "                  f\"val={AMPBConfig.COLOR_VALUE}{len(val_idx)}{AMPBConfig.COLOR_RESET}\", end='')\n",
    "            \n",
    "            # Preparar datos del fold\n",
    "            X_fold_train = X_train.values[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "            X_fold_val = X_train.values[val_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx]\n",
    "            y_fold_val_original = y_train_original.iloc[val_idx]\n",
    "            \n",
    "            # Validar que tenemos suficientes datos para secuencias\n",
    "            if len(X_fold_train) < best_hyperparams['sequence_length'] + 10:\n",
    "                print(f\" [Saltado: datos insuficientes]\")\n",
    "                continue\n",
    "            \n",
    "            # Crear datasets usando tu función existente\n",
    "            fold_train_dataset = createTFDataset(\n",
    "                X_fold_train, y_fold_train, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'], \n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            fold_val_dataset = createTFDataset(\n",
    "                X_fold_val, y_fold_val, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'], \n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Crear y entrenar modelo con los mejores hiperparámetros\n",
    "            tf.keras.backend.clear_session()\n",
    "            fold_model = createModelTransformer(\n",
    "                best_hyperparams['sequence_length'],\n",
    "                X_train.shape[1],\n",
    "                best_hyperparams['d_model'],\n",
    "                best_hyperparams['num_heads'],\n",
    "                best_hyperparams['ff_dim'],\n",
    "                best_hyperparams['num_transformer_blocks'],\n",
    "                best_hyperparams['dropout_rate'],\n",
    "                best_hyperparams['learning_rate']\n",
    "            )            \n",
    "           \n",
    "            # Entrenar el modelo del fold\n",
    "            fold_history = fold_model.fit(\n",
    "                fold_train_dataset,\n",
    "                epochs=fixed_epochs,\n",
    "                validation_data=fold_val_dataset,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Hacer predicciones en el conjunto de validación del fold\n",
    "            fold_val_predictions_scaled = fold_model.predict(fold_val_dataset, verbose=0)\n",
    "            fold_val_predictions_scaled = fold_val_predictions_scaled.flatten()\n",
    "            \n",
    "            # Verificar que tenemos predicciones válidas\n",
    "            if len(fold_val_predictions_scaled) == 0:\n",
    "                print(f\" [Error: sin predicciones]\")\n",
    "                tf.keras.backend.clear_session()\n",
    "                del fold_model\n",
    "                continue\n",
    "            \n",
    "            # Crear series con índices apropiados para des-transformación\n",
    "            available_val_indices = y_fold_val_original.index[:len(fold_val_predictions_scaled)]\n",
    "            fold_predictions_scaled_series = pd.Series(fold_val_predictions_scaled, index=available_val_indices)\n",
    "            \n",
    "            # Des-transformar predicciones usando último valor de entrenamiento como referencia\n",
    "            reference_val = y_train_original.iloc[train_idx[-1]] if len(train_idx) > 0 else y_train_original.iloc[0]\n",
    "            \n",
    "            try:\n",
    "                fold_predictions_original = reverseTransformPredictions(\n",
    "                    fold_predictions_scaled_series,\n",
    "                    reference_val,\n",
    "                    y_scaler,\n",
    "                    transformation, \n",
    "                    params_close,\n",
    "                    prediction_max_limit\n",
    "                )\n",
    "                \n",
    "                # Obtener valores reales correspondientes\n",
    "                y_true_fold = y_fold_val_original.iloc[:len(fold_predictions_original)]\n",
    "                \n",
    "                # Calcular métricas de regresión\n",
    "                from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "                \n",
    "                fold_r2 = r2_score(y_true_fold, fold_predictions_original)\n",
    "                fold_mae = mean_absolute_error(y_true_fold, fold_predictions_original)\n",
    "                fold_rmse = np.sqrt(mean_squared_error(y_true_fold, fold_predictions_original))\n",
    "                \n",
    "                # MÉTRICAS DIRECCIONALES MEJORADAS\n",
    "                # Calcular dirección de movimiento (1 si sube, 0 si baja)\n",
    "                y_true_diff = y_true_fold.diff().dropna()\n",
    "                pred_diff = fold_predictions_original.diff().dropna()\n",
    "                \n",
    "                # Alinear índices en caso de diferencias\n",
    "                common_idx = y_true_diff.index.intersection(pred_diff.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    y_true_diff_aligned = y_true_diff.loc[common_idx]\n",
    "                    pred_diff_aligned = pred_diff.loc[common_idx]\n",
    "                    \n",
    "                    y_true_direction = (y_true_diff_aligned > 0).astype(int)\n",
    "                    pred_direction = (pred_diff_aligned > 0).astype(int)\n",
    "                    \n",
    "                    # Accuracy direccional\n",
    "                    if len(y_true_direction) > 0:\n",
    "                        fold_accuracy = (y_true_direction == pred_direction).mean()\n",
    "                        \n",
    "                        # F1-Score direccional\n",
    "                        from sklearn.metrics import f1_score, roc_auc_score\n",
    "                        try:\n",
    "                            fold_f1_score = f1_score(y_true_direction, pred_direction, zero_division=0)\n",
    "                        except:\n",
    "                            fold_f1_score = 0.0\n",
    "                        \n",
    "                        # ROC-AUC direccional (usando diferencias normalizadas como scores)\n",
    "                        try:\n",
    "                            if len(np.unique(y_true_direction)) > 1:  # Necesitamos ambas clases\n",
    "                                # Normalizar diferencias para usar como probabilidades\n",
    "                                pred_diff_norm = (pred_diff_aligned - pred_diff_aligned.min()) / (pred_diff_aligned.max() - pred_diff_aligned.min())\n",
    "                                pred_diff_norm = pred_diff_norm.fillna(0.5)\n",
    "                                fold_roc_auc = roc_auc_score(y_true_direction, pred_diff_norm)\n",
    "                            else:\n",
    "                                fold_roc_auc = 0.5  # Solo una clase presente\n",
    "                        except:\n",
    "                            fold_roc_auc = 0.5\n",
    "                    else:\n",
    "                        fold_accuracy = 0.0\n",
    "                        fold_f1_score = 0.0\n",
    "                        fold_roc_auc = 0.5\n",
    "                else:\n",
    "                    fold_accuracy = 0.0\n",
    "                    fold_f1_score = 0.0\n",
    "                    fold_roc_auc = 0.5\n",
    "                \n",
    "                # Guardar resultados del fold\n",
    "                epochs_trained = len(fold_history.history['loss'])\n",
    "                cv_results.append({\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'r2': fold_r2,\n",
    "                    'mae': fold_mae,\n",
    "                    'rmse': fold_rmse,\n",
    "                    'accuracy': fold_accuracy,\n",
    "                    'f1_score': fold_f1_score,\n",
    "                    'roc_auc': fold_roc_auc,\n",
    "                    'train_size': len(train_idx),\n",
    "                    'val_size': len(val_idx),\n",
    "                    'epochs_trained': epochs_trained,\n",
    "                    'final_val_loss': min(fold_history.history['val_loss'])\n",
    "                })\n",
    "                \n",
    "                # MOSTRAR RESULTADOS DEL FOLD CON MÁS INFORMACIÓN\n",
    "                print(f\" → R²={fold_r2:.4f}, MAE={fold_mae:.2f}, Acc={fold_accuracy:.3f}, F1={fold_f1_score:.3f}, Épocas={epochs_trained}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" [Error: {str(e)[:50]}...]\")\n",
    "            \n",
    "            # Limpiar memoria\n",
    "            tf.keras.backend.clear_session()\n",
    "            del fold_model\n",
    "        \n",
    "        cv_elapsed = time.time() - cv_start_time\n",
    "        \n",
    "        # USAR LA FUNCIÓN GENÉRICA PARA MOSTRAR ESTADÍSTICAS\n",
    "        cv_metrics = printMetricsCV(cv_results, cv_elapsed, len(cv_folds))\n",
    "\n",
    "else:\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Cross Validation deshabilitado{AMPBConfig.COLOR_RESET}\")\n",
    "    # Valores por defecto si CV está deshabilitado\n",
    "    cv_metrics = {\n",
    "        'cv_mean_r2': sv_r2,\n",
    "        'cv_mean_mae': sv_mae,\n",
    "        'cv_mean_rmse': sv_rmse,\n",
    "        'cv_mean_accuracy': sv_accuracy,\n",
    "        'cv_mean_f1_score': sv_f1_score,\n",
    "        'cv_mean_roc_auc': sv_roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7A. MODO DE BACKTESTING: PREDICCIÓN DÍA A DÍA (VALIDACIÓN BACKTESTING)\n",
    "if run_backtesting:    \n",
    "    model_title_backtest = f'{model_title} (Backtesting [{retrain_interval}d])'\n",
    "    print(f\"\\n{AMPBConfig.COLOR_INFO}Modo Backtesting con Walk-Forward (Retrain cada {retrain_interval} días){AMPBConfig.COLOR_RESET}\")\n",
    "    \n",
    "    # Inicializar historiales\n",
    "    history_y = y_train.copy()              # Datos transformados/escalados para el modelo\n",
    "    history_X = X_train.copy()              # Datos transformados/escalados para el modelo\n",
    "    history_y_original = y_train_original.copy()  # Valores originales para referencias\n",
    "    history_X_original = X_train_original.copy() \n",
    "    \n",
    "    predictions_original_bt = []\n",
    "    model_bt = None  # Modelo que se reutilizará entre reentrenamientos\n",
    "    \n",
    "    bt_start = time.time()\n",
    "    for t in range(len(y_test)):\n",
    "        print(f\" Backtesting: {t+1}/{len(y_test)}\", end='')\n",
    "        \n",
    "        # Reentrenar el modelo cuando sea necesario\n",
    "        if t % retrain_interval == 0:   \n",
    "            print(f\" [Reentrenando...]\", end='')\n",
    "            \n",
    "            val_size = calculateValidationSplit(len(history_y), best_hyperparams['sequence_length'], test_size)\n",
    "            \n",
    "            X_bt_train = history_X.values[:-val_size]\n",
    "            y_bt_train = history_y[:-val_size]\n",
    "            X_bt_val = history_X.values[-val_size:]\n",
    "            y_bt_val = history_y[-val_size:]\n",
    "            \n",
    "            # Crear datasets\n",
    "            bt_train_dataset = createTFDataset(\n",
    "                X_bt_train, y_bt_train, \n",
    "                best_hyperparams['sequence_length'], \n",
    "                best_hyperparams['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            bt_val_dataset = createTFDataset(\n",
    "                X_bt_val, y_bt_val,\n",
    "                best_hyperparams['sequence_length'],\n",
    "                best_hyperparams['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Limpiar sesión y crear modelo\n",
    "            tf.keras.backend.clear_session()            \n",
    "            model_bt = createModelTransformer(\n",
    "                best_hyperparams['sequence_length'],\n",
    "                X_train.shape[1],\n",
    "                best_hyperparams['d_model'],\n",
    "                best_hyperparams['num_heads'],\n",
    "                best_hyperparams['ff_dim'],\n",
    "                best_hyperparams['num_transformer_blocks'],\n",
    "                best_hyperparams['dropout_rate'],\n",
    "                best_hyperparams['learning_rate']\n",
    "            )\n",
    "            \n",
    "            # Entranamiento\n",
    "            model_bt.fit(\n",
    "                bt_train_dataset,\n",
    "                epochs=fixed_epochs,\n",
    "                validation_data=bt_val_dataset,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            print(f\" [✓]\", end='')\n",
    "        \n",
    "        print(f\"\")\n",
    "        \n",
    "        # Generar variables exógenas propagadas para el día t que vamos a predecir\n",
    "        X_current_propagated = updateNextDayExog(\n",
    "            history_X,\n",
    "            feature_original_close=history_y_original.iloc[-1],        # Close_{t-1} original\n",
    "            transformation=transformation,\n",
    "            params_exog=params_exog,\n",
    "            exog_scaler=exog_scaler,\n",
    "            prev_open_original=(history_X_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    "        )\n",
    "        \n",
    "        # Combinar datos históricos + variables exógenas propagadas para crear secuencias. Esto es necesario porque Transformer necesita secuencias completas hasta el día t\n",
    "        current_full_X = np.vstack([history_X.values, X_current_propagated.values])\n",
    "        current_full_y = np.concatenate([history_y.values, [history_y.iloc[-1]]]) \n",
    "        \n",
    "        # Crear secuencias con datos históricos + variables exógenas propagadas\n",
    "        X_current_seq, y_current_seq = createSequences(current_full_X, current_full_y, best_hyperparams['sequence_length'], include_current_day=False)\n",
    "        \n",
    "        # Verificar que tenemos suficientes secuencias\n",
    "        if len(X_current_seq) == 0:\n",
    "            print(f\" [Error: No hay suficientes datos para crear secuencias en t={t}]\")\n",
    "            continue\n",
    "        \n",
    "        # Tomar la última secuencia disponible (que incluye las variables exógenas propagadas)\n",
    "        X_prediction_seq = X_current_seq[-1:]\n",
    "        \n",
    "        # Predecir \n",
    "        forecast_step_scaled_transformed = model_bt.predict(X_prediction_seq, batch_size=best_hyperparams['batch_size'], verbose=0)\n",
    "        forecast_step_scaled_transformed = forecast_step_scaled_transformed.flatten()[0]\n",
    "        forecast_step_scaled_transformed = pd.Series([forecast_step_scaled_transformed], index=[y_test.index[t]])\n",
    "        \n",
    "        # Des-transformar usando la última referencia disponible\n",
    "        reference_val = history_y_original.iloc[-1]\n",
    "        forecast_step_original = reverseTransformPredictions(\n",
    "            forecast_step_scaled_transformed,\n",
    "            reference_val,\n",
    "            y_scaler,\n",
    "            transformation,\n",
    "            params_close,\n",
    "            prediction_max_limit\n",
    "        ).iloc[0]\n",
    "        \n",
    "        # Guardar predicción\n",
    "        predictions_original_bt.append(forecast_step_original)\n",
    "        \n",
    "        # Actualizar historiales con datos reales del día t\n",
    "        history_y = pd.concat([history_y, y_test.iloc[t:t+1]])\n",
    "        history_X = pd.concat([history_X, X_test.iloc[t:t+1]])\n",
    "        history_y_original = pd.concat([history_y_original, pd.Series([y_test_original.iloc[t]], index=[y_test_original.index[t]])])\n",
    "        history_X_original = pd.concat([history_X_original, X_test_original.iloc[t:t+1]])\n",
    "    \n",
    "    # Estadísticas de reentrenamiento\n",
    "    total_retrains = (len(y_test) + retrain_interval - 1) // retrain_interval   \n",
    "    print(f\" Backtesting completado en {time.time() - bt_start:.1f}s\")\n",
    "    print(f\" Reentrenamientos realizados: {total_retrains} (cada {retrain_interval} días)\")\n",
    "    \n",
    "    # Crear Serie con predicciones del backtesting\n",
    "    forecast_backtest_original = pd.Series(predictions_original_bt, index=y_test_original.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d04332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7B. MODO DE BACKTESTING: PREDICCIÓN SIGUIENTE DÍA  \n",
    "if run_backtesting:\n",
    "    # Usar updateNextDayExogNew() para el día siguiente  \n",
    "    X_next_day_bt = updateNextDayExog(\n",
    "        history_X,\n",
    "        feature_original_close=history_y_original.iloc[-1],\n",
    "        transformation=transformation,\n",
    "        params_exog=params_exog,\n",
    "        exog_scaler=exog_scaler,\n",
    "        prev_open_original=(history_X_original['Open'].iloc[-1] if transformation == \"RetLog\" else None)\n",
    "    )\n",
    "\n",
    "    next_day_date = y_test.index[-1] + pd.tseries.offsets.BDay(1)\n",
    "    X_next_day_bt.index = [next_day_date]\n",
    "    \n",
    "    # Crear secuencias finales para predicción del día siguiente\n",
    "    final_full_X = np.vstack([history_X.values, X_next_day_bt.values])\n",
    "    final_full_y = np.concatenate([history_y.values, [history_y.iloc[-1]]])  # Dummy para createSequences\n",
    "    \n",
    "    X_final_seq, y_final_seq = createSequences(final_full_X, final_full_y, best_hyperparams['sequence_length'], include_current_day=False)\n",
    "    \n",
    "    # Tomar la última secuencia\n",
    "    last_sequence = X_final_seq[-1:]\n",
    "    \n",
    "    # Predecir día siguiente\n",
    "    next_forecast_scaled_transformed_bt = model_bt.predict(last_sequence, verbose=0).flatten()[0]\n",
    "    next_forecast_scaled_transformed_bt = pd.Series([next_forecast_scaled_transformed_bt], index=[next_day_date])\n",
    "    \n",
    "    # Des-transformar usando la última referencia del backtesting\n",
    "    next_day_forecast_val_bt_original = reverseTransformPredictions(\n",
    "        next_forecast_scaled_transformed_bt, \n",
    "        history_y_original.iloc[-1],   # Última referencia conocida en backtesting\n",
    "        y_scaler, \n",
    "        transformation, \n",
    "        params_close,\n",
    "        prediction_max_limit\n",
    "    ).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7C. MODO DE BACKTESTING: EVALUACIÓN Y GRÁFICAS \n",
    "if run_backtesting:\n",
    "    # Evaluación del backtesting \n",
    "    bt_r2, bt_mae, bt_rmse, bt_accuracy, bt_f1_score, bt_roc_auc = generateEvaluation(\n",
    "        y_test_original, \n",
    "        forecast_backtest_original, \n",
    "        df_test_aligned, \n",
    "        model_title_backtest, \n",
    "        model_hash,\n",
    "        next_day_date, \n",
    "        next_day_forecast_val_bt_original, \n",
    "        \"Backtesting\"\n",
    "    )\n",
    "\n",
    "    # Guardar informe del backtesting \n",
    "    createReport(\n",
    "        model_name, \n",
    "        \"BT\", \n",
    "        f\"{transformation}_{exog_scaling}_{exog_set_id}\", \n",
    "        model_title_backtest, \n",
    "        model_hash, \n",
    "        bt_r2, \n",
    "        bt_mae, \n",
    "        bt_rmse, \n",
    "        bt_accuracy, \n",
    "        bt_f1_score, \n",
    "        bt_roc_auc\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
